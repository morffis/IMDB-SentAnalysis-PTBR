{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#DATA MANIPULATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#EMBEDDING AND PREPROCESSING\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import gc\n",
    "\n",
    "#TIME CONTROLS\n",
    "import time\n",
    "\n",
    "#PLOT\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "#TENSORFLOW AND KERAS\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional, GlobalAveragePooling1D, MaxPooling1D, Flatten, Masking\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "import random\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename, encoding='utf-8'):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r',encoding=encoding)\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        try:\n",
    "            embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_weight_matrix(embedding, vocab, seq_len):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, seq_len))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix\n",
    "\n",
    "def cleaning(doc):\n",
    "    txt = [token.text for token in doc]\n",
    "    if len(txt) > 2:        \n",
    "        return re.sub(' +', ' ', ' '.join(txt)).strip()\n",
    "\n",
    "def preprocess_string(string, word_vectors):\n",
    "    unk_string = '<unk>'\n",
    "    counter = 0\n",
    "    string = re.sub('[(,).\\\\/\\-_\\+\":“0-9]', ' ', str(string)).lower()\n",
    "    for word in string.split():\n",
    "        try:\n",
    "            word_vectors[word]\n",
    "            string_to_attatch = word\n",
    "        except:\n",
    "            string_to_attatch = unk_string\n",
    "        \n",
    "        if counter:\n",
    "            string = string +' '+ string_to_attatch\n",
    "        else:\n",
    "            string = string_to_attatch\n",
    "            counter = 1\n",
    "    \n",
    "    return string\n",
    "\n",
    "def pad_sequence(string, tokenizer):\n",
    "    encoded_string = tokenizer.texts_to_sequences(string)\n",
    "    padded_enconded = pad_sequences(encoded_string, maxlen=max_length, padding='post')\n",
    "    return padded_enconded\n",
    "\n",
    "def preprocess_to_predict(string, word_vectors, tokenizer):\n",
    "    string = preprocess_string(string, word_vectors)    \n",
    "    padded_sequence = pad_sequence(string, tokenizer)\n",
    "    \n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTANTS\n",
    "MAX_SEQ_LEN = 200 #number of words to consider\n",
    "INPUT_DIMS = 50 #number of dimensions in GLOVE vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "      <th>sentiment_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "      <td>mais uma vez o sr costner arrumou um filme por...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "      <td>este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>primeiro de tudo eu odeio esses raps imbecis q...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>nem mesmo os beatles puderam escrever músicas ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "      <td>filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_en  \\\n",
       "id                                                      \n",
       "1   Once again Mr. Costner has dragged out a movie...   \n",
       "2   This is an example of why the majority of acti...   \n",
       "3   First of all I hate those moronic rappers, who...   \n",
       "4   Not even the Beatles could write songs everyon...   \n",
       "5   Brass pictures movies is not a fitting word fo...   \n",
       "\n",
       "                                              text_pt sentiment  \\\n",
       "id                                                                \n",
       "1   Mais uma vez, o Sr. Costner arrumou um filme p...       neg   \n",
       "2   Este é um exemplo do motivo pelo qual a maiori...       neg   \n",
       "3   Primeiro de tudo eu odeio esses raps imbecis, ...       neg   \n",
       "4   Nem mesmo os Beatles puderam escrever músicas ...       neg   \n",
       "5   Filmes de fotos de latão não é uma palavra apr...       neg   \n",
       "\n",
       "                                                clean  sentiment_code  \n",
       "id                                                                     \n",
       "1   mais uma vez o sr costner arrumou um filme por...               0  \n",
       "2   este é um exemplo do motivo pelo qual a maiori...               0  \n",
       "3   primeiro de tudo eu odeio esses raps imbecis q...               0  \n",
       "4   nem mesmo os beatles puderam escrever músicas ...               0  \n",
       "5   filmes de fotos de latão não é uma palavra apr...               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'imdb-reviews-pt-br.csv',              \n",
    "    index_col=0, \n",
    "    sep=',',\n",
    "    encoding='utf-8', \n",
    "    dtype=str, \n",
    "    quotechar='\"').dropna()\n",
    "\n",
    "raw_embedding = load_embedding('glove_s50.txt')\n",
    "txt = [preprocess_string(doc, raw_embedding) for doc in df['text_pt']]\n",
    "df['clean'] = txt\n",
    "df['sentiment_code'] = np.where(df['sentiment']=='neg', 0, 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = df['clean'].values.tolist()\n",
    "labels = df['sentiment_code'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(filters='!\"#$%&*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "t.fit_on_texts(x_input)\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQ_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   25,    12,    75,     4,   549,  9361, 51120,     6,     8,\n",
       "          21,    27,    25,    72,    15,     2,     4,  1370,   154,\n",
       "          69,   997,  1492,     1,  3326,    22,  1695,    69,   655,\n",
       "          56,    27,  1217,    14,   167,     9,    55,  5942,    16,\n",
       "         203,    34,    78,     5,   200,     1,   173,    46,  2091,\n",
       "          22,  4411,     3,     4,   108, 70823,     7,  3810,   580,\n",
       "          22,   360,     3,    77,  2021,    79,    27,    25,   388,\n",
       "          43,    14,     9,    55,  3087,     4,   108,    16,     4,\n",
       "         263,  5158,    84,  1815,     7,    27,  2609,     3, 51121,\n",
       "       11472, 12074,     4,   355,     7,     2,    23,   851,    18,\n",
       "           6,   594,     2,  1531,     2,     7,    66,    15,     2,\n",
       "          73,   208,   277,    45,    33,   907,     3,     9,   340,\n",
       "        3811,     1,     6,  4411, 16288,    33,   301,  9926,    85,\n",
       "         223, 16289,  9361,   349,    43,  1391,    53,   154,    15,\n",
       "         247,    15,   351,  9361,    84,   643,    41,    19,  2091,\n",
       "          34,    11,  1189,  8987,     1,    21,     2, 12074,     7,\n",
       "        1605,     5,    38,     4,    66,    65, 34028,    37, 32180,\n",
       "        1038,   332,  3278,   117,    47,    74,     2,    14,  1190,\n",
       "          80,    13,     9,  4426,    12,   419,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = get_weight_matrix(raw_embedding, t.word_index, seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index['mais']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.77328902, -0.214645  , -0.94515002, -3.27474403,  0.047547  ,\n",
       "        0.33738399, -0.36342201, -0.624255  , -0.66911697,  1.02033901,\n",
       "        0.62575299,  0.731152  , -0.479958  , -0.30400699,  0.19501901,\n",
       "        0.59655303, -0.299766  , -0.22447699, -0.056082  ,  1.45373595,\n",
       "        1.52957106, -0.033997  , -0.400985  , -0.716034  , -0.87385201,\n",
       "       -0.97609699,  0.617194  , -0.52412701, -0.53311199,  2.09159398,\n",
       "       -0.080944  ,  0.53168398,  0.048488  ,  0.62522602, -0.43868801,\n",
       "       -0.85810298, -0.830791  , -0.159187  , -1.19930506,  0.297562  ,\n",
       "        0.74319702, -0.246746  , -0.096066  ,  0.48618099,  0.620085  ,\n",
       "       -0.40474099,  0.84864199, -0.51519102,  0.103377  , -0.43511599])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vectors[t.word_index['mais']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, INPUT_DIMS, weights=[embedding_vectors], mask_zero=False, input_length=MAX_SEQ_LEN, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([200, 50])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just to check if everything is in order\n",
    "masked_output = e(padded_docs[0])\n",
    "masked_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(masked_output._keras_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#ARCHTECTURE #3\\ntry:\\n    del model\\nexcept:\\n    pass\\nmodel = Sequential([\\n    e,\\n    Bidirectional(LSTM(64,  return_sequences=True)),\\n    Bidirectional(LSTM(32)),\\n    Dense(64, activation='relu'),\\n    Dropout(0.2),\\n    Dense(1)\\n\\n])\\n\\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\\n              optimizer=tf.keras.optimizers.Adam(1e-4),\\n              metrics=['accuracy'])\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#ARCHTECTURE #3\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "model = Sequential([\n",
    "    e,\n",
    "    Bidirectional(LSTM(64,  return_sequences=True)),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARCHTECTURE #4\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "model = Sequential([\n",
    "    e,\n",
    "    Masking(mask_value=0),\n",
    "    #Bidirectional(LSTM(64,  return_sequences=True)),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 50)           4948950   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                21248     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,974,423\n",
      "Trainable params: 25,473\n",
      "Non-trainable params: 4,948,950\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39567 samples, validate on 9892 samples\n",
      "Epoch 1/50\n",
      "24448/39567 [=================>............] - ETA: 8s - loss: 0.6616 - accuracy: 0.6243"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    hist = model.fit(padded_docs, \n",
    "                     labels, \n",
    "                     validation_split=0.2,\n",
    "                     epochs=50,\n",
    "                     batch_size=128, \n",
    "                     shuffle=True,\n",
    "                     verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(hist.history)\n",
    "#plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.plot(history[\"loss\"], 'r',label='loss')\n",
    "plt.plot(history[\"val_loss\"], 'b', label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(hist.history)\n",
    "#plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.plot(history[\"accuracy\"], 'r',label='acc')\n",
    "plt.plot(history[\"val_accuracy\"], 'b', label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('easy_checkpoint')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
